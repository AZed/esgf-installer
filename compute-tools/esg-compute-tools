#!/bin/bash

#####
# esg-compute-tools: ESGF Node Application Stack - Compute Tools
# description: Compute tools installer for the ESGF Node application stack
#
#****************************************************************************
#*                                                                          *
#*    Organization: Lawrence Livermore National Lab (LLNL)                  *
#*    Directorate: Computation                                              *
#*    Department: Computing Applications and Research                       *
#*    Division: S&T Global Security                                         *
#*    Matrix: Atmospheric, Earth and Energy Division                        *
#*    Program: PCMDI                                                        *
#*    Project: Earth Systems Grid Fed (ESGF) Node Software Stack            *
#*    First Author: Eugenia Gabrielova (gabrielov1@llnl.gov)                *
#*                                                                          *
#****************************************************************************
#*                                                                          *
#*   Copyright (c) 2009, Lawrence Livermore National Security, LLC.         *
#*   Produced at the Lawrence Livermore National Laboratory                 *
#*   Written by: Gavin M. Bell (gavin@llnl.gov),                            * 
#*               Eugenia Gabrielova (gabrielov1@llnl.gov)                   *
#*   LLNL-CODE-420962                                                       *
#*                                                                          *
#*   All rights reserved. This file is part of the:                         *
#*   Earth System Grid Fed (ESGF) Node Software Stack, Version 1.0          *
#*                                                                          *
#*   For details, see http://esgf.org/                                      *
#*   Please also read this link                                             *
#*    http://esgf.org/LICENSE                                               *
#*                                                                          *
#*   * Redistribution and use in source and binary forms, with or           *
#*   without modification, are permitted provided that the following        *
#*   conditions are met:                                                    *
#*                                                                          *
#*   * Redistributions of source code must retain the above copyright       *
#*   notice, this list of conditions and the disclaimer below.              *
#*                                                                          *
#*   * Redistributions in binary form must reproduce the above copyright    *
#*   notice, this list of conditions and the disclaimer (as noted below)    *
#*   in the documentation and/or other materials provided with the          *
#*   distribution.                                                          *
#*                                                                          *
#*   Neither the name of the LLNS/LLNL nor the names of its contributors    *
#*   may be used to endorse or promote products derived from this           *
#*   software without specific prior written permission.                    *
#*                                                                          *
#*   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS    *
#*   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT      *
#*   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS      *
#*   FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LAWRENCE    *
#*   LIVERMORE NATIONAL SECURITY, LLC, THE U.S. DEPARTMENT OF ENERGY OR     *
#*   CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,           *
#*   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT       *
#*   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF       *
#*   USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND    *
#*   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,     *
#*   OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT     *
#*   OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF     *
#*   SUCH DAMAGE.                                                           *
#*                                                                          *
#****************************************************************************
#####

#####
# Description: Installer for ESG Compute Tools
# Implemented Tools: Spark, Hadoop, Zookeeper
# Tools in Progress: 
# * Yarn (Hadoop Streaming)
# * Mesos (Cluster Management)
# * Cascalog (Clojure-based query language for Hadoop)
# * Scoobi (Scala framework for Hadoop)

# Authors: Eugenia Gabrielova {gabrielov1@llnl.gov, genia.likes.science@gmail.com}
####

#####
# uses: git, tar, wget
#####

# TODO When missing variables, add them as "use defaults" below --v

#--------------
# User Defined / Settable (public)
#--------------
install_prefix=${install_prefix:-"/usr/local"}
esg_root_dir=${esg_root_dir:-${ESGF_HOME:-"/esg"}}
DEBUG=${DEBUG:-0}
git_placeholder="$install_prefix/git"
git_exec_path_param="--exec-path=$git_placeholder/libexec/git-core"
java_placeholder="$install_prefix/java"
java_install_path_config_param="--with-java-home=$java_placeholder/bin/java"
compress_extensions=".tar.gz|.tar.bz2|.tgz|.bz2|.tar"
envfile="/etc/esg.env"
esg_functions_file=./esg-functions
esg_compute_languages_file=./esg-compute-languages
install_manifest=${install_manifest:-"${esg_root_dir}/esgf-install-manifest"}
# esg_config_dir, create directory called esg_config_compute in esg_config_dir

#--------------------------------
# External programs' versions
#--------------------------------
zookeeper_version=${zookeeper_version:="3.3.5"} #
zookeeper_min_version=${zookeeper_min_version:="3.3.3"}
zookeeper_max_version=${zookeeper_max_version:="3.4.3"}
mesos_version=${mesos_version:="0.9.0"}
mesos_min_version=${mesos_min_version:="0.9.0"}
spark_version=${spark_version:="0.5.0"}
spark_min_version=${spark_min_version:="0.5.0"}
hadoop_version=${hadoop_version:="1.0.3"}
hadoop_min_version=${hadoop_min_version:="1.0.1"}
hadoop_max_version=${hadoop_max_version:="1.0.4"}

#--------------------------------
# External programs' script variables
#--------------------------------
mesos_install_dir=${MESOS_HOME:-${install_prefix}/mesos}
mesos_git_url="git://git.apache.org/mesos.git"
spark_install_dir=${SPARK_HOME:-${install_prefix}/spark}
spark_git_url="git://github.com/mesos/spark.git"
zookeeper_install_dir=${ZOOKEEPER_HOME:-${install_prefix}/zookeeper}
zookeeper_dist_url=http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-${zookeeper_version}/zookeeper-${zookeeper_version}.tar.gz
hadoop_install_dir=${HADOOP_HOME:-${install_prefix}/hadoop}
hadoop_dist_url=http://www.gtlib.gatech.edu/pub/apache/hadoop/common/hadoop-${hadoop_version}/hadoop-${hadoop_version}.tar.gz
hadoop_work_dir=${hadoop_work_dir:-${install_prefix}/hadoop_working}

#-------------------------------
# External programs' configuration variables
#-------------------------------
zookeeper_client_port=2181
zookeeper_workdir=${workdir}/zookeeper

#-------------------------------
# Internal script variables
#-------------------------------
date_format="+%Y_%m_%d_%H%M%S"

[ -e "${envfile}" ] && source ${envfile} && ((VERBOSE)) && printf "sourcing environment from: ${envfile} \n"
[ -e ${esg_functions_file} ] && source ${esg_functions_file} && ((VERBOSE)) && printf "sourcing from: ${esg_functions_file} \n" || \
    (echo "ERROR: Cannot locate functions file ${esg_functions_file}" && exit 1)

#####
# Mesos (Cluster manager for resource sharing across distirbuted applications)
#####
setup_mesos() {
	# Checking Mesos Version
	echo
	echo -n "Checking for Mesos >= ${mesos_min_version}"
	if [ -e ${mesos_install_dir} ]; then
		
		# Unconfigured Mesos will not be able to check its version
		if [ ! -e ${mesos_install_dir}/configure ]; then
			(cd ${mesos_install_dir} && ./bootstrap)
		fi
		# TODO Maybe rebuild Mesos method would be useful

		local mesos_current_version=`${mesos_install_dir}/configure -version | head -1 | awk '{print $3}'`
		check_version_helper $mesos_current_version ${mesos_min_version} 
		[ $? == 0 ] && (( ! force_install )) && echo " [OK]" && return 0
	else
		echo
		echo "No Mesos installation detected"
	fi

	echo
	echo "**********************************"
	echo "Setting Up Mesos ${mesos_version}"
	echo "**********************************"
	echo 

	# Retrieve Mesos Source and build after configuration for ESGF tools
	# Uses: Hadoop, Zookeeper, Java, Python
	git ${git_exec_path_param} clone ${mesos_git_url} ${mesos_install_dir}
	pushd ${mesos_install_dir} >& /dev/null
	./bootstrap
	./configure --with-zookeeper=${zookeeper_install_dir}
	make
	make install
	popd >& /dev/null

	# TODO make check hangs on zookeeper install
	# TODO Check download errors (git failure)
	# Boilerplate: [ $? != 0 ] && echo " ERROR: Could not clone Mesos: ${mesos_git_url}" && popd && checked_done 1
	# TODO Mesos /usr/local configuration
	# TODO ./configure with Hadoop, Zookeeper, Java, Python from /usr/local
    
	# Add Mesos home to environment and install manifest
	write_env_mesos
	write_mesos_install_log_entry
}

reinstall_mesos () {
	echo
}

write_env_mesos() {
	echo "export MESOS_HOME=${mesos_install_dir}" >> ${envfile}
	dedup ${envfile} && source ${envfile}
	return 0
}

write_mesos_install_log_entry() {	
    local entry="$(date ${date_format}) esg-compute-tools:mesos=${mesos_version} ${mesos_install_dir}"
    echo ${entry} >> ${install_manifest}
	dedup ${install_manifest}
	return 0
}

config_mesos() {
    echo
}

test_mesos() {
	# Test the installation of Mesos
	echo "Testing Mesos configuration... "
	local mesosTestConfiguration=$(cd ${mesos_install_dir} && make check && echo $?)
	echo $mesosTestConfiguration

	# Test the language-specific test frameworks
	# Test Hadoop and Spark
}

run_mesos() {
	echo
}

clean_mesos() {
	doit="N"
    if [ -e ${mesos_install_dir} ]; then
        read -e -p "remove cluster management framework Mesos? (${mesos_install_dir}) [y/N]: " doit
        if [ "doit" = "Y" ] || [ "$doit" = "y" ]; then
        	echo "removing ${mesos_install_dir}"
        	rm -rf ${mesos_install_dir}
        	[ $? != 0 ] && echo "ERROR: Unable to remove ${mesos_install_dir}"
            remove_env MESOS_HOME
            remove_install_log_entry mesos
        fi  
    fi  
}

#####
# Spark (Mapreduce framework in Scala, targeted at iterative applications that make use of working
# sets of data)
#####

setup_spark() {
	# Checking for Spark Version
	echo
	echo -n "Checking for Spark >= ${spark_min_version}"
	if [ -e ${spark_install_dir} ]; then
		local spark_current_version=$(cd ${spark_install_dir} && sbt/sbt version | awk 'NR==5{split($3,array,"-")} END{print array[1]}' )
		check_version_helper $spark_current_version ${spark_min_version}
		[ $? == 0 ] && (( ! force_install )) && echo " [OK]" && return 
	else
		echo
		echo "No Spark installation detected"
	fi

    echo
	echo "*****************************"
	echo "Setting Up Spark ${spark_version}"
	echo "*****************************"
	echo

	# Retrieve and build Spark
	git ${git_exec_path_param} clone ${spark_git_url} ${spark_install_dir}
	echo "source ${envfile}" >> ${spark_install_dir}/conf/spark-env.sh
	# TODO Spark Java environment configuration
	(cd ${spark_install_dir} && sbt/sbt compile)
	
	# TODO check for download errors or git failures
	
	# Add Spark home to environment and install manifest
	write_env_spark
	write_spark_install_log_entry
}

write_env_spark() {
	echo "export SPARK_HOME=${spark_install_dir}" >> ${envfile}	
	dedup ${envfile} && source ${envfile}
	return 0
}

write_spark_install_log_entry() {
    local entry="$(date ${date_format}) esg-compute-tools:spark=${spark_version} ${spark_install_dir}"
    echo ${entry} >> ${install_manifest}
	dedup ${install_manifest}
	return 0
}

config_spark() {	
	doconfig="N" 
    if [ -e ${spark_install_dir} ]; then
        echo
		read -e -p "Configure Spark Installation? [y/N]: " doconfig
        
		# Configure Spark environment, deployment, local/Mesos
		if [ "$doconfig" = "Y" ] || [ "$doconfig" = "y" ]; then
        	echo "Configuring Spark..."
       
			# TODO Configure Spark here 
		
		# Inform user of skipped configuration step
		else
			echo "Skipping Spark Configuration..."
		fi
	fi  
}

test_spark() {
	# Test 1: Running a "local" version of Spark with 2 cores
	echo -n "Testing Spark - Local...   "
	local sparkResult=$(cd ${spark_install_dir} && ./run spark.examples.SparkLR local[2] >& /dev/null && echo $?) 
	[ $sparkResult == 0 ] && echo "[ PASSED ]" || echo "[ FAILED ]"
}

run_spark() {
	echo
}

clean_spark() {
	doit="N"
    if [ -e ${spark_install_dir} ]; then
        read -e -p "remove iterative computation framework Spark? (${spark_install_dir}) [y/N]: " doit
        if [ "doit" = "Y" ] || [ "$doit" = "y" ]; then
        	echo "removing ${spark_install_dir}"
        	rm -rf ${spark_install_dir}
        	[ $? != 0 ] && echo "ERROR: Unable to remove ${spark_install_dir}"
            remove_env SPARK_HOME
            remove_install_log_entry spark
        fi  
    fi  
}

#####
# Hadoop (MapReduce distributed computing on HDFS)
#####
setup_hadoop() {
	# Check Hadoop Version
	echo -n "Checking for Hadoop >= ${hadoop_min_version}"
	if [ -e ${hadoop_install_dir} ]; then
        local hadoop_current_version=$(export HADOOP_HOME_WARN_SUPPRESS="TRUE" && \
			${hadoop_install_dir}/bin/hadoop version | head -1 | awk '{print $2}')
		local hadoop_version_number=${hadoop_current_version%-*}
		check_version_helper $hadoop_version_number ${hadoop_min_version} ${hadoop_max_version}
        [ $? == 0 ] && (( ! force_install )) && echo " [OK]" && return 0
    else
    	echo
    	echo "No Hadoop installation detected"
    fi 

    echo
    echo "*****************************"
    echo "Setting up Hadoop ${hadoop_version}"
    echo "*****************************"
    echo

	# Retrieve Hadoop Distribution File
	local hadoop_dist_file=${hadoop_dist_url##*/}
	local hadoop_dist_dir=$(echo ${hadoop_dist_file} | awk 'gsub(/('$compress_extensions')/,"")')

	# Check for empty distribution file (size 0)
	# TODO

	if [ ! -e ${hadoop_dist_dir} ]; then
		echo "Don't see Hadoop distribution directory ${hadoop_dist_dir}"
		wget -O "${install_prefix}/${hadoop_dist_file}" ${hadoop_dist_url}
		[ $? != 0 ] && echo " ERROR: Could not download Hadoop: ${hadoop_dist_file}" && popd && checked_done 1
		echo "Unpacking ${hadoop_dist_file}..."
		tar xzf ${install_prefix}/${hadoop_dist_file} -C ${install_prefix}
		mv "${install_prefix}/${hadoop_dist_dir}" ${hadoop_install_dir}
		mkdir $
		[ $? != 0 ] && echo " ERROR: Could not extract Hadoop: ${hadoop_dist_file}" && popd && checked_done 1
	fi

    # Add Hadoop home to environment and install manifest
	write_env_hadoop
	write_hadoop_install_log_entry

    # Remove Hadoop Distribution File
    if [ -e "${install_prefix}/${hadoop_dist_file}" ]; then
        rm "${install_prefix}/${hadoop_dist_file}"
    fi

	# Create working diretory
	if [ ! -e ${hadoop_work_dir} ]; then
		mkdir ${hadoop_work_dir}
	fi
}

write_env_hadoop() {
	echo "export HADOOP_HOME_WARN_SUPPRESS=\"TRUE\"" >> ${envfile}
	echo "export HADOOP_HOME=${hadoop_install_dir}" >> ${envfile}
	echo "export JAVA_HOME=${install_prefix}/java" >> ${hadoop_install_dir}/conf/hadoop-env.sh
	dedup ${envfile} && source ${envfile}
	return 0
}

write_hadoop_install_log_entry() {	
    local entry="$(date ${date_format}) esg-compute-tools:hadoop=${hadoop_version} ${hadoop_install_dir}"
    echo ${entry} >> ${install_manifest}
	dedup ${install_manifest}
	return 0
}

config_hadoop() {
	# This configuration method should prompt the user to enter settings pertaining to the type
	# of Hadoop distribution (local, semi-distributed, cluster distributed). 
	# There is some info on getting Hadoop setup here: http://hadoop.apache.org/common/#Getting+Started

	# TODO Fully distributed operation requires a pretty hefty amount of configuration. Using the defaults
	# Hadoop provides is probably best. This configuration guide will be used for fully distributed configuration:
	# http://hadoop.apache.org/common/docs/r0.20.2/cluster_setup.html	

	# Initialize configuration
	local doconfig="N" 
    if [ -e ${hadoop_install_dir} ]; then
        read -e -p "Configure Hadoop Installation? [y/N]: " doconfig
        
		# Configure Hadoop modes, HDFS, configuration
		if [ "$doconfig" = "Y" ] || [ "$doconfig" = "y" ]; then
        	echo "Configuring Hadoop..."
		
			# This configures both stand-alone and pseudo-distributed configuration files	
			pseudo_distributed_config_setup

			echo "Configure default Hadoop distribution mode for this node:"
            # Stand-alone operation doesn't really require configuration,
            # as Hadoop just runs as a .jar file. Some examples may require an 
            # input and output directory, but it is probably overkill to configure 
            # that here. Configuring Hadoop to run local should be enough.
            echo "[1] Stand-alone operation (good for debugging)"
			echo "[2] Pseudo-distributed operation on local node"
			read -e -p "(Default: Stand-Alone Operation [2]): " distribution_mode

			# Pseudo-Distributed Local Operation
            if [ "$distribution_mode" = "2" ]; then
				# To learn more about Pseudo-Distributed operation or to fine-tune your configuration,
				# check out the Hadoop documentation here: 
                # http://hadoop.apache.org/common/docs/r0.20.2/quickstart.html#PseudoDistributed	
			
				# TODO: Write configuration to hadoop-config in esg-env here...
				echo "Hadoop: Psuedo-Distributed operation is now default for this node."
            
			# Standalone Local Operation
			else
                
				# TODO: Set standalone as default mode in ESG configuration for hadoop
                echo "Hadoop: Stand-Alone Hadoop Operation is now default for this node."
            fi	
		
		# Inform user of skipped configuration step
		else
			echo "Skipping Hadoop Configuration..."
		fi
	fi  
}

start_hadoop_local_distributed() {
    # TODO Start Hadoop with configuration files from ESG configuration directory
    echo "Hadoop: Starting hadoop in local distributed mode..."
    ${hadoop_install_dir}/bin/start-all.sh --config ${hadoop_install_dir}/conf/local_distributed >& /dev/null
}
	
stop_hadoop_local_distributed() {
	echo "Hadoop: Shutting down namenode and jobtracker..."
    ${hadoop_install_dir}/bin/stop-all.sh >& /dev/null	
}

pseudo_distributed_config_setup() {

	# TODO Best practice would be to not keep these guys in Hadoop's install tree...
	# Create directories for standalone, local_distributed configuration
	# For now we destroy them first.
	rm -rf ${hadoop_install_dir}/conf/standalone
	rm -rf ${hadoop_install_dir}/conf/local_distributed
	mkdir ${hadoop_install_dir}/conf/standalone 
	mkdir ${hadoop_install_dir}/conf/local_distributed

	# Copy default example config files to new directory for standalone	
	cp ${hadoop_install_dir}/conf/*.xml ${hadoop_install_dir}/conf/standalone/
	cp ${hadoop_install_dir}/conf/hadoop-env.sh ${hadoop_install_dir}/conf/standalone/hadoop-env.sh
	cp ${hadoop_install_dir}/conf/slaves ${hadoop_install_dir}/conf/standalone/slaves
	cp ${hadoop_install_dir}/conf/hadoop-env.sh ${hadoop_install_dir}/conf/local_distributed/hadoop-env.sh
	cp ${hadoop_install_dir}/conf/slaves ${hadoop_install_dir}/conf/local_distributed/slaves

	# Create configuration files for pseudo-distributed mode
	echo '<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:9000</value>
	</property>
	</configuration>' >> ${hadoop_install_dir}/conf/local_distributed/core-site.xml

	echo '<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
	<property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    </configuration>' >> ${hadoop_install_dir}/conf/local_distributed/hdfs-site.xml

	echo '<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
    <property>
    <name>mapred.job.tracker</name>
    <value>localhost:9001</value>
    </property>
    </configuration>' >> ${hadoop_install_dir}/conf/local_distributed/mapred-site.xml
}

test_hadoop() {
    # Hadoop Test - Local (Standalone) Mode
    test_hadoop_standalone
	
    # Hadoop Test 2 - Local Psuedo-Distributed
	test_hadoop_pseudo_distributed
}

test_hadoop_standalone() {

	# Hadoop Test - Local (Standalone) Mode, with wordcount example
	echo -n "Testing Hadoop - Local (Standalone) Mode...   "
	
	mkdir ${hadoop_install_dir}/sandbox
	pushd ${hadoop_install_dir} >& /dev/null
	
	mkdir sandbox/input
	cp conf/*.xml sandbox/input
	bin/hadoop jar hadoop-examples-*.jar grep sandbox/input sandbox/output 'dfs[a-z.]+' >& /dev/null
	[ -e sandbox/output/_SUCCESS ] && echo "[ PASSED ]" || echo "[ FAILED ]"
	
	popd >& /dev/null
	rm -rf ${hadoop_install_dir}/sandbox
}

hadoop_temp_local_ssh () {
	# Setup a local ssh: this functionality is duplicated elsewhere in esg-node
	# installer but is needed temporarily for hadoop pseudo-distribution
	if [ ! -e ~/.ssh/id_dsa ]; then
		ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
		cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	fi
}

test_hadoop_pseudo_distributed() {
	
	# Generate temporary ssh keys
	hadoop_temp_local_ssh

	echo "Begin Testing Hadoop - Pseudo-Distributed Mode...   "	
    if [ -e ${hadoop_install_dir}/conf/local_distributed ]; then
		#${hadoop_install_dir}/bin/hadoop namenode -format
		start_hadoop_local_distributed

		${hadoop_install_dir}/bin/hadoop fs -mkdir ${hadoop_work_dir}/input >& /dev/null
		${hadoop_install_dir}/bin/hadoop fs -put ${hadoop_install_dir}/conf/local_distributed/* ${hadoop_work_dir}/input >& /dev/null
		# Fix hard-coding yo...
		${hadoop_install_dir}/bin/hadoop jar ${hadoop_install_dir}/hadoop-examples-*.jar grep ${hadoop_work_dir}/input ${hadoop_work_dir}/output 'dfs[a-z.]+' >& /dev/null
		echo -n "Testing Hadoop - Pseudo-Distributed Mode...   "
		[ -e ${hadoop_work_dir}/output/_SUCCESS ] && echo "[ PASSED ]" || echo "[ FAILED ]"
		${hadoop_install_dir}/bin/hadoop fs -rmr ${hadoop_work_dir}/input >& /dev/null
		${hadoop_install_dir}/bin/hadoop fs -rmr ${hadoop_work_dir}/output >& /dev/null
    
		stop_hadoop_local_distributed

	else
		echo
        echo "Hadoop not configured for Pseudo-distributed mode. Skipping test."
    fi
}

clean_hadoop() {
	doit="N"
    if [ -e ${hadoop_install_dir} ]; then
        read -e -p "remove mapreduce framework Hadoop? (${hadoop_install_dir}) [y/N]: " doit
        if [ "doit" = "Y" ] || [ "$doit" = "y" ]; then
        	echo "removing ${hadoop_install_dir}"
        	rm -rf ${hadoop_install_dir}
        	[ $? != 0 ] && echo "ERROR: Unable to remove ${hadoop_install_dir}"
            remove_env HADOOP_HOME
            remove_install_log_entry hadoop
        fi  
    fi  
}

#####
# Zookeeper (Synchronization and quorum manager for clusters)
#####

setup_zookeeper() {
	# Check ZooKeeper Version
	echo
	echo -n "Checking for Zookeeper >= ${zookeeper_min_version}"
	if [ -e ${zookeeper_install_dir} ] && [ -x ${zookeeper_install_dir}/bin/zkServer.sh ]; then	
	    local zookeeper_current_version=$(/bin/ls ${zookeeper_install_dir} | egrep '^zookeeper.*jar$' | sed 's/[^0-9.]*\([0-9.]*\)\..*/\1/')
		check_version_helper $zookeeper_current_version ${zookeeper_min_version} ${zookeeper_max_version}
        [ $? == 0 ] && (( ! force_install )) && echo " [OK]" && return 0
	else
		echo
		echo "No Zookeeper installation detected"
	fi

    echo
    echo "*****************************"
    echo "Setting up Zookeeper ${hadoop_version}"
    echo "*****************************"
    echo

	# Retrieve Zookeeper Distribution File
	local zookeeper_dist_file=${zookeeper_dist_url##*/}
	local zookeeper_dist_dir=$(echo ${zookeeper_dist_file} | awk 'gsub(/('$compress_extensions')/,"")')

	# Check for empty distribution file (size 0)
	# TODO

	if [ ! -e ${zookeeper_dist_dir} ]; then
		echo "Don't see Zookeeper distribution directory ${zookeeper_dist_dir}"
		wget -O "${install_prefix}/${zookeeper_dist_file}" ${zookeeper_dist_url}
		[ $? != 0 ] && echo " ERROR: Could not download Zookeeper: ${zookeeper_dist_file}" && popd && checked_done 1
		echo "Unpacking ${zookeeper_dist_file}..."
		tar xzf ${install_prefix}/${zookeeper_dist_file} -C ${install_prefix}
		mv "${install_prefix}/${zookeeper_dist_dir}" ${zookeeper_install_dir}
		[ $? != 0 ] && echo " ERROR: Could not extract Zookeeper: ${zookeeper_dist_file}" && popd && checked_done 1
	fi

	# Configure Zookeeper environment (this is temporary)
	mkdir ${zookeeper_install_dir}/zookeeper_working_sandbox 
	echo "tickTime=2000" >> ${zookeeper_install_dir}/conf/zoo.cfg
	echo "dataDir=${zookeeper_install_dir}/zookeeper_working_sandbox" >> ${zookeeper_install_dir}/conf/zoo.cfg
	echo "clientPort=2181" >> ${zookeeper_install_dir}/conf/zoo.cfg

    # Add Zookeeper home to environment and install manifest
	write_env_zookeeper
	write_zookeeper_install_log_entry
}

write_env_zookeeper() {	
	echo "export ZOOKEEPER_HOME=${zookeeper_install_dir}" >> ${envfile}
	dedup ${envfile} && source ${envfile}
	return 0
}

write_zookeeper_install_log_entry() {
    local entry="$(date ${date_format}) esg-compute-tools:zookeeper=${zookeeper_version} ${zookeeper_install_dir}"
    echo ${entry} >> ${install_manifest}
	dedup ${install_manifest}
	return 0
}

config_zookeeper() {
	doconfig="N" 
    if [ -e ${zookeeper_install_dir} ]; then
        echo
		read -e -p "Configure Zookeeper Installation? [y/N]: " doconfig
        
		# Configure Hadoop modes, HDFS, configuration
		if [ "$doconfig" = "Y" ] || [ "$doconfig" = "y" ]; then
        	echo "Configuring Zookeeper..."
       
			# TODO Configure Zookeeper here 
		
		# Inform user of skipped configuration step
		else
			echo "Skipping Zookeeper Configuration..."
		fi
	fi  
}

test_zookeeper() {
	# Basic zookeeper test for client connection	
	echo -n "Testing Zookeeper client connection...   "

	${zookeeper_install_dir}/bin/zkServer.sh start >& /dev/null
    ${zookeeper_install_dir}/bin/zkCli.sh -server 127.0.0.1:2181 ls / quit >& /dev/null
    zkRet=$?
    [ ${zkRet} == 0 ] && echo " [ PASSED ]" || echo " [ FAILED ]"
    ${zookeeper_install_dir}/bin/zkServer.sh stop >& /dev/null
	return ${zkRet}
}

clean_zookeeper() {
	doit="N"
    if [ -e ${zookeeper_install_dir} ]; then
        read -e -p "remove cluster management tool Zookeeper? (${zookeeper_install_dir}) [y/N]: " doit
        if [ "doit" = "Y" ] || [ "$doit" = "y" ]; then
        	echo "removing ${zookeeper_install_dir}"
        	rm -rf ${zookeeper_install_dir}
        	[ $? != 0 ] && echo "ERROR: Unable to remove ${zookeeper_install_dir}"
            remove_env ZOOKEEPER_HOME
            remove_install_log_entry zookeeper
        fi  
    fi  
}

: << '--COMMENT--'
zookeeper_workdir=${workdir}/zookeeper
zookeeper_data_dir=${esg_root_dir}/zookeeper

setup_zookeeper() {
    
	mkdir -p ${zookeeper_workdir}
    pushd ${zookeeper_workdir} >& /dev/null

    local zookeeper_dist_file=${zookeeper_dist_url##*/}
    local zookeeper_dist_dir=$(echo ${zookeeper_dist_file} | awk 'gsub(/('$compress_extensions')/,"")')

    #There is this pesky case of having a zero sized dist file... WTF!?
    if [ -e ${zookeeper_dist_file} ]; then
        ls -l ${zookeeper_dist_file}
        local size=$(stat -c%s ${zookeeper_dist_file})
        (( size == 0 )) && rm -v ${zookeeper_dist_file}
    fi

    #Check to see if we already have a zookeeper distribution directory
    if [ ! -e ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} ]; then
        echo "Don't see zookeeper distribution dir ${zookeeper_install_dir%/*}/${zookeeper_dist_dir}"
        if [ ! -e ${zookeeper_dist_file} ]; then
            echo "Don't see zookeeper distribution file $(pwd)/${zookeeper_dist_file} either"
            echo "Downloading zookeeper from ${zookeeper_dist_url}"
            #NOTE: this should be a checked_get call!
            #When I do the refactoring of some of the functions, then I can source that functions file and use checked_get.
            #For now just fetch it.
            wget -O ${zookeeper_dist_file} ${zookeeper_dist_url}
            [ $? != 0 ] && echo " ERROR: Could not download zookeeper ${zookeeper_dist_file}" && popd && checked_done 1
            echo "unpacking ${zookeeper_dist_file}... into ${zookeeper_install_dir%/*}"
            tar xzf ${zookeeper_dist_file} -C ${zookeeper_install_dir%/*}
            [ $? != 0 ] && echo " ERROR: Could not extract zookeeper :-( " && popd && checked_done 1
        fi
    fi
    
    #If you don't see the directory but see the tar.gz distribution
    #then expand it and go from there....
    if [ -e ${zookeeper_dist_file} ] && [ ! -e ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} ]; then
        echo "unpacking* ${zookeeper_dist_file} into ${zookeeper_install_dir%/*}"
        tar xzf ${zookeeper_dist_file} -C ${zookeeper_install_dir%/*}
        [ $? != 0 ] && echo " ERROR: Could not extract zookeeper :-( " && popd && checked_done 1
    fi

    if [ ! -e ${zookeeper_install_dir} ]; then
        ln -s ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} ${zookeeper_install_dir}
        [ $? != 0 ] && \
            echo " ERROR: Could not create sym link ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} -> ${zookeeper_install_dir}" && popd && checked_done 1
    else
        unlink ${zookeeper_install_dir}
        [ $? != 0 ] && mv ${zookeeper_install_dir} ${zookeeper_install_dir}.$(date ${date_format}).bak
        
        ln -s ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} ${zookeeper_install_dir}
        [ $? != 0 ] && \
            echo " ERROR*: Could not create sym link ${zookeeper_install_dir%/*}/${zookeeper_dist_dir} -> ${zookeeper_install_dir}" && popd && checked_done 1
    fi
    (($DEBUG)) && echo "chown -R ${installer_uid}:${installer_gid} ${zookeeper_install_dir}"
    chown    ${installer_uid}:${installer_gid} ${zookeeper_install_dir}
    chown -R ${installer_uid}:${installer_gid} $(readlink -f ${zookeeper_install_dir})
    
    popd >& /dev/null
    echo "zookeeper setup [OK]"
    return 0
}

configure_zookeeper() {
    echo -n "Configuring zookeeper... $@"
    pushd ${zookeeper_install_dir} >& /dev/null
    [ $? != 0 ] && echo " ERROR: Unable to peform configuration: no such dir ${zookeeper_install_dir}" && checked_done 1

    #new hotness....
    local config_file="${zookeeper_install_dir}/conf/zoo.cfg"
    local config_info=""
    local key="${1:-dataDir}"
    local value="${2:-${zookeeper_data_dir}}"

    #Look for the local config file first.  If it is there and the key is present then do a local replacement of the value with ${zookeeper_data_dir}
    #If the file is not there OR this particular key is not present, pull the distribution config file down and to the replacement based on that content.

    if [ -f "${config_file}" ] && (grep "${key}" "${config_file}" >& /dev/null) && (( ! $force_install )); then
        config_info=$(cat "${config_file}")
    else 
        echo "fetching base config file ${config_file##*/} from distribution server..."
        config_info=$(curl -s -L --insecure ${esg_dist_url}/esg-search/zookeeper/${config_file##*/})
        (($force_install)) && cp ${config_file} ${config_file}.last
    fi
    
    (($DEBUG)) && echo "(pre) Configuration Infos:" && echo "${config_info}"

    #Now, do the value replacement based on matching against the KEY not the VALUE! (more flexible and robust)
    sed 's#\('${key}'=\)\(.*\)#\1'${value}'#g'  <(echo "${config_info}") >> ${config_file}

    #dedup the configuration file
    local tmp=$(tac ${config_file} | awk 'BEGIN {FS="="} !($1 in a) {a[$1];print $0}' | sort -k1,1)
    echo "$tmp" > ${config_file}

    
    (($DEBUG)) && echo "(post) Configuration Info:" && cat ${config_file}
    #-------------

    #Set this node's zookeeper ID
    mkdir -p ${zookeeper_data_dir}
    get_node_id > ${zookeeper_data_dir}/myid
    chown    ${installer_uid}:${installer_gid} ${zookeeper_data_dir}
    chown -R ${installer_uid}:${installer_gid} $(readlink -f ${zookeeper_data_dir})
    echo -n "zookeeper id: $(cat ${zookeeper_data_dir}/myid)"
    popd >& /dev/null
    echo " [OK]"
    return 0
}

start_zookeeper() {

    check_zookeeper_process && return 1
    echo "Starting zookeeper server on port ${zookeeper_client_port}"

    ${zookeeper_install_dir}/bin/zkServer.sh start

    #-----
    #NOTE: (potential timing issue)
    #Luca: the wait is because Solr, on startup, needs to send information to Zookeeper
    #  not sure if it is enough to wait for the client port to be accessible, we'll test
    #-----

    #Don't wait 10 seconds if you don't have to...
    #just check every second up till 10 seconds (or <= 10x)
    local wait_time=10
    local ret=1
    while [[ $wait_time > 0 ]]; do
        netstat -na | grep -i ${zookeeper_client_port}
        ret=$?
        [ $ret == 0 ] && break
        sleep 1
        ((wait_time--))
        echo -n "."
    done
    [ $ret == 0 ] && echo " [OK]" || echo " [FAIL]"
    return $ret
}

stop_zookeeper() {
    echo "Stopping zookeeper..."
    ${zookeeper_install_dir}/bin/zkServer.sh stop
    local ret=$?
    [ ${ret} == 0 ] && echo " [OK]" || echo " [FAIL]"
    return ${ret}
}

#status
check_zookeeper_process() {
    local pid=`lsof -i:${zookeeper_client_port} | grep -i java | awk '{print $2}'`
    [ -n "$pid" ] && echo " Zookeeper process running on port [${zookeeper_client_port}]... " && return 0
}

--COMMENT--

#####
# Core Methods
#####

clean_compute_tools() {
	# TODO: A few of these computation frameworks rely on one another,
	# so uninstalling portions of this suite may leave parts unstable. Some
	# sort of sanity check would be useful here.

	clean_spark
	clean_mesos
	clean_hadoop
	clean_zookeeper
}

test_compute_tools() {
	# Method to run test suites of all compute tools.

	echo
	echo "---------------------------------"
	echo " Testing ESGF Node Compute Tools "
	echo "---------------------------------"
	echo

	# Test Hadoop Single-node
	test_hadoop

	# Test Zookeeper Local
	test_zookeeper

	# Test Mesos Local placeholder

	# Test Spark Local
	test_spark
}

config_compute_tools() {
	# Method to run configuration methods of all compute tools.
	
	echo
	echo "---------------------------------"
	echo " Configuring ESGF Node Compute Tools "
	echo "---------------------------------"
	echo

	config_hadoop
	config_zookeeper
	config_spark
}

setup_compute_tools() {
	echo
	echo "-------------------------------------------------------------"
	echo "Installing ESGF Node Compute Tools {Hadoop, Zookeeper, Spark}"
	echo "-------------------------------------------------------------"
	echo

	# The optimal install order, as below, depends on the following:
	# 1. Hadoop
	# 2. Zookeeper
	# 3. Spark (Requires Mesos for distributed computation)
	# Future: Mesos, Yarn (currently on Hadoop Master), Cascalog (Clojure query language for Hadoop)
	
	setup_hadoop
	setup_zookeeper
	setup_spark

	config_compute_tools
	test_compute_tools
}

if [[ "$BASH_SOURCE" == "$0" ]]
then
	echo
	echo "----- Setting Up Compute Tools -----"
	setup_compute_tools
fi
